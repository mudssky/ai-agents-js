{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  \"id\": \"d46edae1-a058-4901-81d2-8fa933639624\",\n",
       "  \"content\": \"有一天，小明去参加一个面试。面试官问他：“你有什么特长吗？”\\n\\n小明想了想，认真地回答：“我会预测未来。”\\n\\n面试官很感兴趣，问：“哦？那你能预测一下你什么时候能被录用吗？”\\n\\n小明微微一笑，说：“这个嘛……我预测我不会被录用。”\\n\\n面试官愣了一下，笑着说：“你的预测很准，你确实不会被录用。”\\n\\n小明耸耸肩：“看，我说对了吧！”\",\n",
       "  \"additional_kwargs\": {},\n",
       "  \"response_metadata\": {\n",
       "    \"tokenUsage\": {\n",
       "      \"promptTokens\": 6,\n",
       "      \"completionTokens\": 85,\n",
       "      \"totalTokens\": 91\n",
       "    },\n",
       "    \"finish_reason\": \"stop\",\n",
       "    \"model_name\": \"deepseek-chat\",\n",
       "    \"usage\": {\n",
       "      \"prompt_tokens\": 6,\n",
       "      \"completion_tokens\": 85,\n",
       "      \"total_tokens\": 91,\n",
       "      \"prompt_tokens_details\": {\n",
       "        \"cached_tokens\": 0\n",
       "      },\n",
       "      \"prompt_cache_hit_tokens\": 0,\n",
       "      \"prompt_cache_miss_tokens\": 6\n",
       "    },\n",
       "    \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
       "  },\n",
       "  \"tool_calls\": [],\n",
       "  \"invalid_tool_calls\": [],\n",
       "  \"usage_metadata\": {\n",
       "    \"output_tokens\": 85,\n",
       "    \"input_tokens\": 6,\n",
       "    \"total_tokens\": 91,\n",
       "    \"input_token_details\": {\n",
       "      \"cache_read\": 0\n",
       "    },\n",
       "    \"output_token_details\": {}\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatDeepSeek } from '@langchain/deepseek';\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const model = new ChatDeepSeek(\n",
    "    {\n",
    "        model:'deepseek-chat'\n",
    "    }\n",
    ");\n",
    "\n",
    "await model.invoke([\n",
    "    new HumanMessage(\"讲个笑话\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"当然可以！这是一个经典的笑话：\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"有一天，小明去参加一个面试。面试官问他：“你有什么特长吗？”\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"小明想了想，认真地说：“我会模仿动物。”\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"面试官有点好奇，说：“那你模仿一下狗吧。”\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"小明立刻蹲下来，开始“汪汪汪”地叫。面试官点点头，说：“不错，那你会模仿猫吗？”\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"小明又立刻趴在地上，开始“喵喵喵”地叫。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"面试官笑了笑，接着问：“那你还会模仿什么动物？”\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"小明想了想，突然站起来，拍了拍桌子，大声说：“老板，我要加薪！”\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"面试官：…… 😄\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// StringOutputParser 提取api返回的文本数据\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const parser = new StringOutputParser();\n",
    "\n",
    "\n",
    "const chain = model.pipe(parser)\n",
    "\n",
    "await chain.invoke([\n",
    "    new HumanMessage(\"讲个笑话\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 部分 Parser 会内置一些预先设计好的 prompt 对模型进行引导\n",
    "import { StructuredOutputParser } from \"langchain/output_parsers\";\n",
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const parser = StructuredOutputParser.fromNamesAndDescriptions({\n",
    "  answer: \"用户问题的答案\",\n",
    "  evidence: \"你回答用户问题所依据的答案\",\n",
    "  confidence: \"问题答案的可信度评分，格式是百分数\",\n",
    "});\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
      "\n",
      "\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
      "\n",
      "For example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\n",
      "would match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\n",
      "Thus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n",
      "\n",
      "Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!\n",
      "\n",
      "Here is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:\n",
      "```json\n",
      "{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"用户问题的答案\"},\"evidence\":{\"type\":\"string\",\"description\":\"你回答用户问题所依据的答案\"},\"confidence\":{\"type\":\"string\",\"description\":\"问题答案的可信度评分，格式是百分数\"}},\"required\":[\"answer\",\"evidence\",\"confidence\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "console.log(parser.getFormatInstructions())\n",
    "// 先告诉 LLM 输出的类型\n",
    "// 然后，使用 few-shot （一种 prompt 技巧），也就是用示例告诉 LLM 什么是 JSON Schema，什么情况会被解析成功，什么情况不会被解析成功\n",
    "// 然后，再次强调类型的重要性，输出必须遵循给定的JSON Schema实例，确保所有字段严格匹配Schema中的定义，没有额外的属性，也没有遗漏的必需属性。\n",
    "// 并强调需要注意细节，比如不要在JSON对象中添加多余的逗号，这可能会导致解析失败。\n",
    "// 这些 prompt 质量非常高，把在该任务中大模型容易出现的问题都进行了强调，可以有效的保证输出的质量。\n",
    "// 最后才是给出，我们指定的 JSON 格式和对应的描述\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  answer: \"蒙娜丽莎的作者是列奥纳多·达·芬奇，绘制时间大约在1503年至1506年之间。\",\n",
      "  evidence: \"蒙娜丽莎是世界上最著名的肖像画之一，其作者列奥纳多·达·芬奇是文艺复兴时期的杰出艺术家。根据历史记录和艺术史研究，达·芬奇在1503年至1506年间创作了这幅画。\",\n",
      "  confidence: \"95%\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const prompt = PromptTemplate.fromTemplate(\"尽可能的回答用的问题 \\n{instructions} \\n{question}\")\n",
    "// const model = new ChatDeekSeek();\n",
    "\n",
    "const chain = prompt.pipe(model).pipe(parser)\n",
    "const res = await chain.invoke({\n",
    "    question: \"蒙娜丽莎的作者是谁？是什么时候绘制的\",\n",
    "    instructions: parser.getFormatInstructions()\n",
    "})\n",
    "                               \n",
    "console.log(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "// List Output Parser,限制列表格式得到指令\n",
    "import { CommaSeparatedListOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const parser = new CommaSeparatedListOutputParser();\n",
    "\n",
    "console.log(parser.getFormatInstructions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// const model = new ChatOpenAI();\n",
    "const prompt = PromptTemplate.fromTemplate(\"列出3个 {country} 的著名的互联网公司.\\n{instructions}\")\n",
    "    \n",
    "const chain = prompt.pipe(model).pipe(parser)\n",
    "\n",
    "const response = await chain.invoke({\n",
    "    country: \"America\",\n",
    "    instructions: parser.getFormatInstructions(),\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ \u001b[32m\"Google\"\u001b[39m, \u001b[32m\"Amazon\"\u001b[39m, \u001b[32m\"Facebook\"\u001b[39m ]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ answer: \"蒙娜丽莎的作者是列奥纳多·达·芬奇，大约在1503年至1506年间绘制。\", confidence: 95 }\n"
     ]
    }
   ],
   "source": [
    "\n",
    "// Auto Fix Parser ，对于部分对输出质量要求更高的场景，如果出现了输出不符合要求的情况，我们希望的不是让 LLM 反复输出（可能每次都是错的），因为 LLM 并没有意识到自己的错误。所以我们需要把报错的信息返回给 LLM，让他理解错在哪里，应该怎么修改。\n",
    "\n",
    "import { z } from \"zod\";\n",
    "const schema = z.object({\n",
    "  answer:  z.string().describe(\"用户问题的答案\"),\n",
    "  confidence: z.number().min(0).max(100).describe(\"问题答案的可信度评分，满分 100\")\n",
    "});\n",
    "const parser = StructuredOutputParser.fromZodSchema(schema);\n",
    "const prompt = PromptTemplate.fromTemplate(\"尽可能的回答用的问题 \\n{instructions} \\n{question}\")\n",
    "// const model = new ChatOpenAI();\n",
    "\n",
    "const chain = prompt.pipe(model).pipe(parser)\n",
    "const res = await chain.invoke({\n",
    "    question: \"蒙娜丽莎的作者是谁？是什么时候绘制的\",\n",
    "    instructions: parser.getFormatInstructions()\n",
    "})\n",
    "                               \n",
    "console.log(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  OutputFixingParser: [class OutputFixingParser extends BaseOutputParser]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{ answer: \u001b[32m\"蒙娜丽莎的作者是达芬奇，大约在16世纪初期（1503年至1506年之间）开始绘制。\"\u001b[39m, confidence: \u001b[33m90\u001b[39m }"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "// fixParse只是一个纯粹的json更合适处理，可以修复json格式的问题。\n",
    "import { OutputFixingParser } from \"langchain/output_parsers\"\n",
    "// 我们构造一个错误的输出，\n",
    "const wrongOutput = {\n",
    "  \"answer\": \"蒙娜丽莎的作者是达芬奇，大约在16世纪初期（1503年至1506年之间）开始绘制。\",\n",
    "  \"sources\": \"90%\" \n",
    "};\n",
    "console.log({OutputFixingParser});\n",
    "\n",
    "\n",
    "const fixParser = OutputFixingParser.fromLLM(model, parser);\n",
    "const output = await fixParser.parse(JSON.stringify(wrongOutput));\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 出于节约成本考虑，可以实现gpt4的错误输出，用3.5来修复，可以节约成本"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
