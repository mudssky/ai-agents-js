{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  response: \u001b[32m\"ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©ã€‚ğŸ˜Š\"\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// è¿™æ˜¯ä¸æ”¯æŒLCELçš„ï¼Œæ‰€ä»¥å®šåˆ¶ä¼šæ¯”è¾ƒéº»çƒ¦ã€‚ä½†æ˜¯å¯ä»¥æ–¹ä¾¿åœ°å®ç°è®°å¿†å¯¹è¯ã€‚\n",
    "import {deepSeekChatModel} from './models/index.ts'\n",
    "import { BufferMemory } from \"langchain/memory\";\n",
    "import { ConversationChain } from \"langchain/chains\";\n",
    "\n",
    "const memory = new BufferMemory();\n",
    "const chain = new ConversationChain({ llm: deepSeekChatModel, memory: memory });\n",
    "const res1 = await chain.call({ input: \"æˆ‘æ˜¯å°æ˜\" });\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ response: \u001b[32m\"ä½ åˆšæ‰å‘Šè¯‰æˆ‘ä½ å«å°æ˜å‘€ï¼ğŸ˜Š å¦‚æœä½ æœ‰å…¶ä»–åå­—æˆ–è€…æ˜µç§°ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘å“¦ï¼\"\u001b[39m }"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const res2 = await chain.call({ input: \"æˆ‘å«ä»€ä¹ˆï¼Ÿ\" });\n",
    "res2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å†…ç½®Memoryçš„æœºåˆ¶  \n",
    "é¦–å…ˆæ˜¯ BufferWindowMemoryï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BufferWindowMemory is not a constructor",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: BufferWindowMemory is not a constructor",
      "    at <anonymous>:5:16",
      "    at eventLoopTick (ext:core/01_core.js:177:7)"
     ]
    }
   ],
   "source": [
    "// å¯¼å…¥BufferWindowMemory\n",
    "// è¿™é‡Œéå¸¸å¥½ç†è§£ï¼Œå°±æ˜¯å¯¹èŠå¤©è®°å½•åŠ äº†ä¸€ä¸ªæ»‘åŠ¨çª—å£ï¼Œåªä¼šè®°å¿† k ä¸ªå¯¹è¯\n",
    "// å¯ä»¥èŠ‚çœtoken\n",
    "import { BufferWindowMemory } from \"@langchain/core/memory\";\n",
    "const memory = new BufferWindowMemory({ k: 1 });\n",
    "const chain = new ConversationChain({ llm: model, memory: memory });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationSummaryMemory,å®˜ç½‘æä¾›çš„ï¼Œéšç€èŠå¤©ä¸æ–­ç”ŸæˆèŠå¤©è®°å½•æ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ConversationSummaryMemory } from \"langchain/memory\";\n",
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatDeepSeek } from \"@langchain/deepseek\";\n",
    "const memory = new ConversationSummaryMemory({\n",
    "    memoryKey: \"summary\",\n",
    "    llm: new ChatDeepSeek({\n",
    "        model: \"deepseek-chat\", \n",
    "        verbose: true,\n",
    "    }),\n",
    "  });\n",
    "\n",
    "const model = deepSeekChatModel;\n",
    "const prompt = PromptTemplate.fromTemplate(`\n",
    "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚å°½ä½ æ‰€èƒ½å›ç­”æ‰€æœ‰é—®é¢˜ã€‚\n",
    "\n",
    "è¿™æ˜¯èŠå¤©è®°å½•çš„æ‘˜è¦:\n",
    "{summary}\n",
    "Human: {input}\n",
    "AI:`);\n",
    "const chain = new ConversationChain({ llm: model, prompt, memory, verbose: true });\n",
    "\n",
    "const res1 = await chain.call({ input: \"æˆ‘æ˜¯å°æ˜\" });\n",
    "const res2 = await chain.call({ input: \"æˆ‘å«ä»€ä¹ˆï¼Ÿ\" });\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°† BufferWindowMemory å’Œ ConversationSummaryMemory ç»“åˆèµ·æ¥ï¼Œæ ¹æ® token æ•°é‡ï¼Œå¦‚æœä¸Šä¸‹æ–‡å†å²è¿‡å¤§æ—¶å°±åˆ‡æ¢åˆ° summaryï¼Œå¦‚æœä¸Šä¸‹æ–‡æ¯”è¾ƒå°æ—¶å°±ä½¿ç”¨åŸå§‹çš„èŠå¤©è®°å½•ï¼Œä¹Ÿå°±æˆäº† ConversationSummaryBufferMemoryã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[chain/start]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] Entering Chain run with input: {\n",
      "  \"input\": \"æˆ‘æ˜¯å°æ˜\",\n",
      "  \"history\": \"\"\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatDeepSeek\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n\\nHuman: æˆ‘æ˜¯å°æ˜\\nAI:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatDeepSeek\u001b[22m\u001b[39m] [10.01s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©å“¦ï¼ğŸ˜Š\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©å“¦ï¼ğŸ˜Š\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 63,\n",
      "                \"completionTokens\": 35,\n",
      "                \"totalTokens\": 98\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 63,\n",
      "                \"completion_tokens\": 35,\n",
      "                \"total_tokens\": 98,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 0\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 0,\n",
      "                \"prompt_cache_miss_tokens\": 63\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
      "            },\n",
      "            \"id\": \"04e18cf6-52ee-4a32-a4d3-62658a5741b1\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 35,\n",
      "              \"input_tokens\": 63,\n",
      "              \"total_tokens\": 98,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 63,\n",
      "      \"completionTokens\": 35,\n",
      "      \"totalTokens\": 98\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to calculate number of tokens, falling back to approximate count Error: Unknown model\n",
      "    at getEncodingNameForModel (file:///C:/home/Projects/AI/ai-agents/node_modules/js-tiktoken/dist/chunk-Z5MDQTGX.js:247:13)\n",
      "    at encodingForModel (file:///C:/home/Projects/AI/ai-agents/node_modules/@langchain/core/dist/utils/tiktoken.js:19:24)\n",
      "    at ChatDeepSeek.getNumTokens (file:///C:/home/Projects/AI/ai-agents/node_modules/@langchain/core/dist/language_models/base.js:197:40)\n",
      "    at ConversationSummaryBufferMemory.prune (file:///C:/home/Projects/AI/ai-agents/node_modules/langchain/dist/memory/summary_buffer.js:114:47)\n",
      "    at eventLoopTick (ext:core/01_core.js:177:7)\n",
      "    at async ConversationSummaryBufferMemory.saveContext (file:///C:/home/Projects/AI/ai-agents/node_modules/langchain/dist/memory/summary_buffer.js:96:9)\n",
      "    at async ConversationChain.invoke (file:///C:/home/Projects/AI/ai-agents/node_modules/langchain/dist/chains/base.js:71:13)\n",
      "    at async <anonymous>:17:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] [10.03s] Exiting Chain run with output: {\n",
      "  \"response\": \"ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©å“¦ï¼ğŸ˜Š\"\n",
      "}\n",
      "{\n",
      "  res1: {\n",
      "    response: \"ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©å“¦ï¼ğŸ˜Š\"\n",
      "  }\n",
      "}\n",
      "\u001b[32m[chain/start]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] Entering Chain run with input: {\n",
      "  \"input\": \"æˆ‘å«ä»€ä¹ˆï¼Ÿ\",\n",
      "  \"history\": \"Human: æˆ‘æ˜¯å°æ˜\\nAI: ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©å“¦ï¼ğŸ˜Š\"\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatDeepSeek\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\nHuman: æˆ‘æ˜¯å°æ˜\\nAI: ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©å“¦ï¼ğŸ˜Š\\nHuman: æˆ‘å«ä»€ä¹ˆï¼Ÿ\\nAI:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatDeepSeek\u001b[22m\u001b[39m] [10.85s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"ä½ åˆšåˆšå‘Šè¯‰æˆ‘ä½ å«å°æ˜å‘€ï¼ğŸ˜Š å¦‚æœä½ æœ‰å…¶ä»–åå­—æˆ–è€…æ˜µç§°ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘å“¦ï¼\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"ä½ åˆšåˆšå‘Šè¯‰æˆ‘ä½ å«å°æ˜å‘€ï¼ğŸ˜Š å¦‚æœä½ æœ‰å…¶ä»–åå­—æˆ–è€…æ˜µç§°ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘å“¦ï¼\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 108,\n",
      "                \"completionTokens\": 22,\n",
      "                \"totalTokens\": 130\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 108,\n",
      "                \"completion_tokens\": 22,\n",
      "                \"total_tokens\": 130,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 64\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 64,\n",
      "                \"prompt_cache_miss_tokens\": 44\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
      "            },\n",
      "            \"id\": \"b90e308c-a71d-44c8-897e-bc3430804681\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 22,\n",
      "              \"input_tokens\": 108,\n",
      "              \"total_tokens\": 130,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 64\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 108,\n",
      "      \"completionTokens\": 22,\n",
      "      \"totalTokens\": 130\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to calculate number of tokens, falling back to approximate count Error: Unknown model\n",
      "    at getEncodingNameForModel (file:///C:/home/Projects/AI/ai-agents/node_modules/js-tiktoken/dist/chunk-Z5MDQTGX.js:247:13)\n",
      "    at encodingForModel (file:///C:/home/Projects/AI/ai-agents/node_modules/@langchain/core/dist/utils/tiktoken.js:19:24)\n",
      "    at ChatDeepSeek.getNumTokens (file:///C:/home/Projects/AI/ai-agents/node_modules/@langchain/core/dist/language_models/base.js:197:40)\n",
      "    at ConversationSummaryBufferMemory.prune (file:///C:/home/Projects/AI/ai-agents/node_modules/langchain/dist/memory/summary_buffer.js:114:47)\n",
      "    at eventLoopTick (ext:core/01_core.js:177:7)\n",
      "    at async ConversationSummaryBufferMemory.saveContext (file:///C:/home/Projects/AI/ai-agents/node_modules/langchain/dist/memory/summary_buffer.js:96:9)\n",
      "    at async ConversationChain.invoke (file:///C:/home/Projects/AI/ai-agents/node_modules/langchain/dist/chains/base.js:71:13)\n",
      "    at async <anonymous>:23:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] [10.85s] Exiting Chain run with output: {\n",
      "  \"response\": \"ä½ åˆšåˆšå‘Šè¯‰æˆ‘ä½ å«å°æ˜å‘€ï¼ğŸ˜Š å¦‚æœä½ æœ‰å…¶ä»–åå­—æˆ–è€…æ˜µç§°ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘å“¦ï¼\"\n",
      "}\n",
      "{ res2: { response: \"ä½ åˆšåˆšå‘Šè¯‰æˆ‘ä½ å«å°æ˜å‘€ï¼ğŸ˜Š å¦‚æœä½ æœ‰å…¶ä»–åå­—æˆ–è€…æ˜µç§°ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘å“¦ï¼\" } }\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import { ConversationSummaryBufferMemory } from \"langchain/memory\";\n",
    "import { ConversationChain } from \"langchain/chains\";\n",
    "import {ChatDeepSeek} from '@langchain/deepseek'\n",
    "\n",
    "// ç›®å‰ä¸æ”¯æŒdeepseekæ¨¡å‹çš„tokenæ•°è®¡ç®—\n",
    "const model = deepSeekChatModel;\n",
    "const memory = new ConversationSummaryBufferMemory({\n",
    "  llm: new ChatDeepSeek({\n",
    "        model: \"deepseek-chat\", \n",
    "        verbose: true,\n",
    "    }),\n",
    "  maxTokenLimit: 200\n",
    "});\n",
    "const chain = new ConversationChain({ llm: model, memory: memory, verbose: true });\n",
    "const res1 = await chain.call({ input: \"æˆ‘æ˜¯å°æ˜\" });\n",
    "console.log({res1});\n",
    "const res2 = await chain.call({ input: \"æˆ‘å«ä»€ä¹ˆï¼Ÿ\" });\n",
    "console.log({res2});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " EntityMemory\n",
    "\n",
    "åœ¨äººç±»èŠå¤©çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å®é™…åœ¨å»ºç«‹çš„æ˜¯å¯¹å„ç§å®ä½“ï¼ˆEntityï¼‰çš„è®°å¿†ï¼Œä¾‹å¦‚ä¸¤ä¸ªåˆšè®¤è¯†çš„äººï¼Œæˆ‘ä»¬èŠèŒä¸šã€èŠå…¬å¸ã€èŠé¤é¦†ï¼Œæˆ‘ä»¬è®°å¿†ä¸­å­˜å‚¨æ–¹å¼å¯èƒ½æ˜¯æ ¹æ®å®ä½“è¿›è¡Œåˆ†ç±»å­˜å‚¨ï¼Œè¿™ä¸ªäººæ˜¯ä»€ä¹ˆèŒä¸šã€å¹´é¾„ï¼›è¿™ä¸ªå…¬å¸æ˜¯ä»€ä¹ˆæƒ…å†µï¼›é¤é¦†æ˜¯ä»€ä¹ˆç¯å¢ƒå’Œå‘³é“ã€‚EntityMemory å¸Œæœ›æ¨¡æ‹Ÿçš„å°±æ˜¯åœ¨èŠå¤©ä¸­å»ç”Ÿæˆå’Œæ›´æ–°ä¸åŒçš„å®ä½“çš„æè¿°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \\\"What do you know about him\\\" where \\\"him\\\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: my name is Jacob. how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Jacob,Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\n\\nLast line of conversation (for extraction):\\nHuman: æˆ‘æ˜¯å°æ˜\\n\\nOutput:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] [7.76s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"å°æ˜\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"å°æ˜\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 409,\n",
      "                \"completionTokens\": 1,\n",
      "                \"totalTokens\": 410\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 409,\n",
      "                \"completion_tokens\": 1,\n",
      "                \"total_tokens\": 410,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 0\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 0,\n",
      "                \"prompt_cache_miss_tokens\": 409\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
      "            },\n",
      "            \"id\": \"52c3f79c-4fb3-40ad-8c98-7c8dda26048b\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 1,\n",
      "              \"input_tokens\": 409,\n",
      "              \"total_tokens\": 410,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 409,\n",
      "      \"completionTokens\": 1,\n",
      "      \"totalTokens\": 410\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[chain/start]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] Entering Chain run with input: {\n",
      "  \"input\": \"æˆ‘æ˜¯å°æ˜\",\n",
      "  \"history\": \"\",\n",
      "  \"entities\": {\n",
      "    \"å°æ˜\": \"No current information known.\"\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatDeepSeek\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an assistant to a human, powered by a large language model trained by OpenAI.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n{\\\"å°æ˜\\\":\\\"No current information known.\\\"}\\n\\nCurrent conversation:\\n\\nLast line:\\nHuman: æˆ‘æ˜¯å°æ˜\\nYou:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatDeepSeek\u001b[22m\u001b[39m] [9.58s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 274,\n",
      "                \"completionTokens\": 32,\n",
      "                \"totalTokens\": 306\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 274,\n",
      "                \"completion_tokens\": 32,\n",
      "                \"total_tokens\": 306,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 0\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 0,\n",
      "                \"prompt_cache_miss_tokens\": 274\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
      "            },\n",
      "            \"id\": \"f014ae94-b845-4f97-8a8c-267544603451\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 32,\n",
      "              \"input_tokens\": 274,\n",
      "              \"total_tokens\": 306,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 274,\n",
      "      \"completionTokens\": 32,\n",
      "      \"totalTokens\": 306\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update and add to the summary of the provided entity in the \\\"Entity\\\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), output the exact string \\\"UNCHANGED\\\" below.\\n\\nFull conversation history (for context):\\nHuman: æˆ‘æ˜¯å°æ˜\\nAI: ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\\n\\nEntity to summarize:\\nå°æ˜\\n\\nExisting summary of å°æ˜:\\nNo current information known.\\n\\nLast line of conversation:\\nHuman: æˆ‘æ˜¯å°æ˜\\nUpdated summary (or the exact string \\\"UNCHANGED\\\" if there is no new information about å°æ˜ above):\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] [7.87s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"å°æ˜ is a person who introduced themselves as \\\"å°æ˜\\\" in the conversation.\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"å°æ˜ is a person who introduced themselves as \\\"å°æ˜\\\" in the conversation.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 244,\n",
      "                \"completionTokens\": 15,\n",
      "                \"totalTokens\": 259\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 244,\n",
      "                \"completion_tokens\": 15,\n",
      "                \"total_tokens\": 259,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 0\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 0,\n",
      "                \"prompt_cache_miss_tokens\": 244\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
      "            },\n",
      "            \"id\": \"d5f509ce-29b9-47bd-ba6c-d9a6f5b9f74b\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 15,\n",
      "              \"input_tokens\": 244,\n",
      "              \"total_tokens\": 259,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 244,\n",
      "      \"completionTokens\": 15,\n",
      "      \"totalTokens\": 259\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] [17.45s] Exiting Chain run with output: {\n",
      "  \"response\": \"ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\"\n",
      "}\n",
      "{\n",
      "  res1: {\n",
      "    response: \"ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\"\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \\\"What do you know about him\\\" where \\\"him\\\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: my name is Jacob. how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Jacob,Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\nHuman: æˆ‘æ˜¯å°æ˜\\nAI: ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\\nLast line of conversation (for extraction):\\nHuman: æˆ‘å«ä»€ä¹ˆï¼Ÿ\\n\\nOutput:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] [9.16s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"NONE  \\n\\nExplanation: The last line \\\"æˆ‘å«ä»€ä¹ˆï¼Ÿ\\\" translates to \\\"What is my name?\\\" and does not contain any proper nouns. The name \\\"å°æ˜\\\" (Xiao Ming) was mentioned earlier but is not in the last line, so it is not included.\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"NONE  \\n\\nExplanation: The last line \\\"æˆ‘å«ä»€ä¹ˆï¼Ÿ\\\" translates to \\\"What is my name?\\\" and does not contain any proper nouns. The name \\\"å°æ˜\\\" (Xiao Ming) was mentioned earlier but is not in the last line, so it is not included.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 451,\n",
      "                \"completionTokens\": 56,\n",
      "                \"totalTokens\": 507\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 451,\n",
      "                \"completion_tokens\": 56,\n",
      "                \"total_tokens\": 507,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 384\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 384,\n",
      "                \"prompt_cache_miss_tokens\": 67\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
      "            },\n",
      "            \"id\": \"9802a5b1-3c14-401f-a4b1-e5ffee262830\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 56,\n",
      "              \"input_tokens\": 451,\n",
      "              \"total_tokens\": 507,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 384\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 451,\n",
      "      \"completionTokens\": 56,\n",
      "      \"totalTokens\": 507\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[chain/start]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] Entering Chain run with input: {\n",
      "  \"input\": \"æˆ‘å«ä»€ä¹ˆï¼Ÿ\",\n",
      "  \"history\": \"Human: æˆ‘æ˜¯å°æ˜\\nAI: ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\",\n",
      "  \"entities\": {\n",
      "    \"NONE  \\n\\nExplanation: The last line \\\"æˆ‘å«ä»€ä¹ˆï¼Ÿ\\\" translates to \\\"What is my name?\\\" and does not contain any proper nouns. The name \\\"å°æ˜\\\" (Xiao Ming) was mentioned earlier but is not in the last line\": \"No current information known.\",\n",
      "    \"so it is not included.\": \"No current information known.\"\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatDeepSeek\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an assistant to a human, powered by a large language model trained by OpenAI.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n{\\\"NONE  \\\\n\\\\nExplanation: The last line \\\\\\\"æˆ‘å«ä»€ä¹ˆï¼Ÿ\\\\\\\" translates to \\\\\\\"What is my name?\\\\\\\" and does not contain any proper nouns. The name \\\\\\\"å°æ˜\\\\\\\" (Xiao Ming) was mentioned earlier but is not in the last line\\\":\\\"No current information known.\\\",\\\"so it is not included.\\\":\\\"No current information known.\\\"}\\n\\nCurrent conversation:\\nHuman: æˆ‘æ˜¯å°æ˜\\nAI: ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\\nLast line:\\nHuman: æˆ‘å«ä»€ä¹ˆï¼Ÿ\\nYou:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatDeepSeek\u001b[22m\u001b[39m] [8.40s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"ä½ å«å°æ˜ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ çš„å—ï¼Ÿ\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"ä½ å«å°æ˜ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ çš„å—ï¼Ÿ\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 381,\n",
      "                \"completionTokens\": 14,\n",
      "                \"totalTokens\": 395\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 381,\n",
      "                \"completion_tokens\": 14,\n",
      "                \"total_tokens\": 395,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 192\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 192,\n",
      "                \"prompt_cache_miss_tokens\": 189\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
      "            },\n",
      "            \"id\": \"1d63be85-3231-49b1-ba47-f096a9a63005\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 14,\n",
      "              \"input_tokens\": 381,\n",
      "              \"total_tokens\": 395,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 192\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 381,\n",
      "      \"completionTokens\": 14,\n",
      "      \"totalTokens\": 395\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update and add to the summary of the provided entity in the \\\"Entity\\\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), output the exact string \\\"UNCHANGED\\\" below.\\n\\nFull conversation history (for context):\\nHuman: æˆ‘æ˜¯å°æ˜\\nAI: ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\\nHuman: æˆ‘å«ä»€ä¹ˆï¼Ÿ\\nAI: ä½ å«å°æ˜ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ çš„å—ï¼Ÿ\\n\\nEntity to summarize:\\nNONE  \\n\\nExplanation: The last line \\\"æˆ‘å«ä»€ä¹ˆï¼Ÿ\\\" translates to \\\"What is my name?\\\" and does not contain any proper nouns. The name \\\"å°æ˜\\\" (Xiao Ming) was mentioned earlier but is not in the last line\\n\\nExisting summary of NONE  \\n\\nExplanation: The last line \\\"æˆ‘å«ä»€ä¹ˆï¼Ÿ\\\" translates to \\\"What is my name?\\\" and does not contain any proper nouns. The name \\\"å°æ˜\\\" (Xiao Ming) was mentioned earlier but is not in the last line:\\nNo current information known.\\n\\nLast line of conversation:\\nHuman: æˆ‘å«ä»€ä¹ˆï¼Ÿ\\nUpdated summary (or the exact string \\\"UNCHANGED\\\" if there is no new information about NONE  \\n\\nExplanation: The last line \\\"æˆ‘å«ä»€ä¹ˆï¼Ÿ\\\" translates to \\\"What is my name?\\\" and does not contain any proper nouns. The name \\\"å°æ˜\\\" (Xiao Ming) was mentioned earlier but is not in the last line above):\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] [9.23s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"UNCHANGED  \\n\\nExplanation: The last line \\\"æˆ‘å«ä»€ä¹ˆï¼Ÿ\\\" translates to \\\"What is my name?\\\" and does not contain any proper nouns. The name \\\"å°æ˜\\\" (Xiao Ming) was mentioned earlier but is not in the last line. Therefore, there is no new information to update the summary.\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"UNCHANGED  \\n\\nExplanation: The last line \\\"æˆ‘å«ä»€ä¹ˆï¼Ÿ\\\" translates to \\\"What is my name?\\\" and does not contain any proper nouns. The name \\\"å°æ˜\\\" (Xiao Ming) was mentioned earlier but is not in the last line. Therefore, there is no new information to update the summary.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 409,\n",
      "                \"completionTokens\": 64,\n",
      "                \"totalTokens\": 473\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 409,\n",
      "                \"completion_tokens\": 64,\n",
      "                \"total_tokens\": 473,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 128\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 128,\n",
      "                \"prompt_cache_miss_tokens\": 281\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
      "            },\n",
      "            \"id\": \"386c1ce3-99cd-4996-95f4-14cd7301f37a\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 64,\n",
      "              \"input_tokens\": 409,\n",
      "              \"total_tokens\": 473,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 128\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 409,\n",
      "      \"completionTokens\": 64,\n",
      "      \"totalTokens\": 473\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update and add to the summary of the provided entity in the \\\"Entity\\\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), output the exact string \\\"UNCHANGED\\\" below.\\n\\nFull conversation history (for context):\\nHuman: æˆ‘æ˜¯å°æ˜\\nAI: ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®å¿™çš„å—ï¼Ÿæ— è®ºæ˜¯å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯è®¨è®ºæŸä¸ªè¯é¢˜ï¼Œæˆ‘éƒ½å¾ˆä¹æ„ååŠ©ä½ ã€‚ğŸ˜Š\\nHuman: æˆ‘å«ä»€ä¹ˆï¼Ÿ\\nAI: ä½ å«å°æ˜ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ çš„å—ï¼Ÿ\\n\\nEntity to summarize:\\nso it is not included.\\n\\nExisting summary of so it is not included.:\\nNo current information known.\\n\\nLast line of conversation:\\nHuman: æˆ‘å«ä»€ä¹ˆï¼Ÿ\\nUpdated summary (or the exact string \\\"UNCHANGED\\\" if there is no new information about so it is not included. above):\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatDeepSeek\u001b[22m\u001b[39m] [13.92s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"UNCHANGED\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"UNCHANGED\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"promptTokens\": 279,\n",
      "                \"completionTokens\": 4,\n",
      "                \"totalTokens\": 283\n",
      "              },\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"deepseek-chat\",\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 279,\n",
      "                \"completion_tokens\": 4,\n",
      "                \"total_tokens\": 283,\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"cached_tokens\": 192\n",
      "                },\n",
      "                \"prompt_cache_hit_tokens\": 192,\n",
      "                \"prompt_cache_miss_tokens\": 87\n",
      "              },\n",
      "              \"system_fingerprint\": \"fp_3a5770e1b4_prod0225\"\n",
      "            },\n",
      "            \"id\": \"e1fa243a-6fab-4ca7-8ce8-2fd62eb2496d\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": [],\n",
      "            \"usage_metadata\": {\n",
      "              \"output_tokens\": 4,\n",
      "              \"input_tokens\": 279,\n",
      "              \"total_tokens\": 283,\n",
      "              \"input_token_details\": {\n",
      "                \"cache_read\": 192\n",
      "              },\n",
      "              \"output_token_details\": {}\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 279,\n",
      "      \"completionTokens\": 4,\n",
      "      \"totalTokens\": 283\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] [31.55s] Exiting Chain run with output: {\n",
      "  \"response\": \"ä½ å«å°æ˜ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ çš„å—ï¼Ÿ\"\n",
      "}\n",
      "{ res2: { response: \"ä½ å«å°æ˜ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ çš„å—ï¼Ÿ\" } }\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import { EntityMemory, ENTITY_MEMORY_CONVERSATION_TEMPLATE } from \"langchain/memory\";\n",
    "import { ConversationChain } from \"langchain/chains\";\n",
    "\n",
    "const model = deepSeekChatModel;\n",
    "const memory = new EntityMemory({\n",
    "    llm: new ChatDeepSeek({\n",
    "        model: \"deepseek-chat\",\n",
    "        verbose: true \n",
    "    }),\n",
    "    chatHistoryKey: \"history\",\n",
    "    entitiesKey: \"entities\"\n",
    "});\n",
    "const chain = new ConversationChain({ \n",
    "    llm: model, \n",
    "    prompt: ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory: memory, \n",
    "    verbose: true \n",
    "});\n",
    "\n",
    "const res1 = await chain.call({ input: \"æˆ‘æ˜¯å°æ˜\" });\n",
    "console.log({res1});\n",
    "const res2 = await chain.call({ input: \"æˆ‘å«ä»€ä¹ˆï¼Ÿ\" });\n",
    "console.log({res2});"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
