{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { AIMessage, HumanMessage } from \"@langchain/core/messages\";\n",
    "const history = new ChatMessageHistory();\n",
    "await history.addMessage(new HumanMessage(\"hi\"));\n",
    "await history.addMessage(new AIMessage(\"What can I do for you?\"));\n",
    "const messages = await history.getMessages();\n",
    "messages;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```typescript\n",
    "export abstract class BaseChatMessageHistory extends Serializable {\n",
    "  public abstract getMessages(): Promise<BaseMessage[]>;\n",
    "\n",
    "  public abstract addMessage(message: BaseMessage): Promise<void>;\n",
    "\n",
    "  public abstract addUserMessage(message: string): Promise<void>;\n",
    "\n",
    "  public abstract addAIChatMessage(message: string): Promise<void>;\n",
    "\n",
    "  public abstract clear(): Promise<void>;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 手动维护\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "import { ChatDeepSeek } from \"@langchain/deepseek\";\n",
    "\n",
    "const chatModel = new ChatDeepSeek({\n",
    "  model: \"deepseek-chat\",\n",
    "});\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "    You are talkative and provides lots of specific details from its context. \n",
    "    If the you does not know the answer to a question, it truthfully says you do not know.`,\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "]);\n",
    "\n",
    "const chain = prompt.pipe(chatModel);\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "await history.addMessage(new HumanMessage(\"hi, my name is Kai\"));\n",
    "const res1 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "});\n",
    "res1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 这里把对话的结果也手动添加到历史记录中\n",
    "await history.addMessage(res1);\n",
    "await history.addMessage(new HumanMessage(\"What is my name?\"));\n",
    "const res2 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "});\n",
    "res2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableWithMessageHistory 有几个参数：\n",
    "\n",
    "- runnable 就是需要被包裹的 chain，可以是任意 chain\n",
    "- getMessageHistory 接收一个函数，函数需要根据传入的 _sessionId，去获取对应的\n",
    "  ChatMessageHistory 对象，这里我们没有 session 管理，所以就返回默认的对象\n",
    "- inputMessagesKey 用户传入的信息 key 的名称，因为 RunnableWithMessageHistory\n",
    "  要自动记录用户和 llm 发送的信息，所以需要在这里声明用户以什么 key 传入信息\n",
    "- historyMessagesKey，聊天记录在 prompt 中的 key，因为要自动的把聊天记录注入到\n",
    "  prompt 中。 outputMessagesKey，因为我们的 chain\n",
    "  只有一个输出就省略了，如果有多个输出需要指定哪个是 llm\n",
    "  的回复，也就是需要存储的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 自动维护chat history，由 RunnableWithMessageHistory 给任意 chain 包裹一层，就能添加聊天记录管理的能力\n",
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "const chatModel = new ChatDeepSeek({\n",
    "  model: \"deepseek-chat\",\n",
    "});\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "  [\"human\", \"{input}\"],\n",
    "]);\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "const chain = prompt.pipe(chatModel);\n",
    "\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: chain,\n",
    "  getMessageHistory: (_sessionId) => history,\n",
    "  inputMessagesKey: \"input\",\n",
    "  historyMessagesKey: \"history_message\",\n",
    "});\n",
    "const res1 = await chainWithHistory.invoke({\n",
    "  input: \"hi, my name is Kai\",\n",
    "}, {\n",
    "  configurable: { sessionId: \"none\" },\n",
    "});\n",
    "res1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const res2 = await chainWithHistory.invoke({\n",
    "  input: \"我的名字叫什么？\",\n",
    "}, {\n",
    "  configurable: { sessionId: \"none\" },\n",
    "});\n",
    "res2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.getMessages();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// RunnableWithMessageHistory 是将历史记录完整的传递到 llm中，我们可以对 llm 的历史记录进行更多操作，例如只传递最近的 k 条历史记录等。\n",
    "\n",
    "// 下面是一个总结历史记录的chain接收两个参数，\n",
    "// summary，上一次总结的信息\n",
    "// new_lines，用户和 llm 新的回复\n",
    "import { RunnableSequence } from \"@langchain/core/runnables\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const summaryModel = new ChatDeepSeek({\n",
    "  model: \"deepseek-chat\",\n",
    "});\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\n",
    "`);\n",
    "\n",
    "const summaryChain = RunnableSequence.from([\n",
    "  summaryPrompt,\n",
    "  summaryModel,\n",
    "  new StringOutputParser(),\n",
    "]);\n",
    "\n",
    "const newSummary = await summaryChain.invoke({\n",
    "  summary: \"\",\n",
    "  new_lines: \"I'm 18\",\n",
    "});\n",
    "await summaryChain.invoke({\n",
    "  summary: newSummary,\n",
    "  new_lines: \"I'm male\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用 new RunnablePassthrough({func: (input)=> void})，是有两个目的：\n",
    "\n",
    "- 如果我们只写 new RunnablePassthrough()，那就是把用户输入的 input\n",
    "  再传递到下一个 runnable 节点中，不做任何操作。因为 RunnableMap\n",
    "  返回值是对其中每个 chain 执行，然后将返回值作为结果传递给下一个 runnable\n",
    "  节点，如果我们不对 input 使用 RunnablePassthrough 则下个节点就拿不到 input\n",
    "  的值\n",
    "- new RunnablePassthrough({func: (input)=> void}) 中的 func 函数是在传递 input\n",
    "  的过程中，执行一个函数，这个函数返回值是\n",
    "  void，也就是无论其内容是什么，都不会对 input 造成影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "import { getBufferString } from \"langchain/memory\";\n",
    "const chatModel = new ChatDeepSeek({\n",
    "  model: \"deepseek-chat\",\n",
    "});\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "\n",
    "    Here is the chat history summary:\n",
    "    {history_summary}\n",
    "    `,\n",
    "  ],\n",
    "  [\"human\", \"{input}\"],\n",
    "]);\n",
    "let summary = \"\";\n",
    "const history = new ChatMessageHistory();\n",
    "\n",
    "const chatChain = RunnableSequence.from([\n",
    "  {\n",
    "    input: new RunnablePassthrough({\n",
    "      func: (input) => history.addUserMessage(input),\n",
    "    }),\n",
    "  },\n",
    "  RunnablePassthrough.assign({\n",
    "    history_summary: () => summary,\n",
    "  }),\n",
    "  chatPrompt,\n",
    "  chatModel,\n",
    "  new StringOutputParser(),\n",
    "  new RunnablePassthrough({\n",
    "    func: async (input) => {\n",
    "      history.addAIChatMessage(input);\n",
    "      const messages = await history.getMessages();\n",
    "      const new_lines = getBufferString(messages);\n",
    "      const newSummary = await summaryChain.invoke({\n",
    "        summary,\n",
    "        new_lines,\n",
    "      });\n",
    "      history.clear();\n",
    "      summary = newSummary;\n",
    "    },\n",
    "  }),\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chatChain.invoke(\"我现在饿了\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chatChain.invoke(\"我今天想吃方便面\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
